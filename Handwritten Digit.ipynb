{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d777c6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400 images belonging to 10 classes.\n",
      "Found 100 images belonging to 10 classes.\n",
      "Epoch 1/40\n",
      "12/12 [==============================] - 3s 128ms/step - loss: 2.3104 - accuracy: 0.1087 - val_loss: 2.2987 - val_accuracy: 0.1042\n",
      "Epoch 2/40\n",
      "12/12 [==============================] - 1s 104ms/step - loss: 2.2956 - accuracy: 0.1114 - val_loss: 2.2896 - val_accuracy: 0.1562\n",
      "Epoch 3/40\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 2.2815 - accuracy: 0.0978 - val_loss: 2.2607 - val_accuracy: 0.1667\n",
      "Epoch 4/40\n",
      "12/12 [==============================] - 1s 114ms/step - loss: 2.2587 - accuracy: 0.1250 - val_loss: 2.2389 - val_accuracy: 0.1458\n",
      "Epoch 5/40\n",
      "12/12 [==============================] - 1s 105ms/step - loss: 2.2097 - accuracy: 0.2092 - val_loss: 2.2671 - val_accuracy: 0.1146\n",
      "Epoch 6/40\n",
      "12/12 [==============================] - 1s 105ms/step - loss: 2.1875 - accuracy: 0.1712 - val_loss: 2.1996 - val_accuracy: 0.0938\n",
      "Epoch 7/40\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 2.1287 - accuracy: 0.2174 - val_loss: 2.1669 - val_accuracy: 0.2604\n",
      "Epoch 8/40\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 2.0313 - accuracy: 0.2690 - val_loss: 2.4109 - val_accuracy: 0.1771\n",
      "Epoch 9/40\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 2.0368 - accuracy: 0.2500 - val_loss: 2.1212 - val_accuracy: 0.2604\n",
      "Epoch 10/40\n",
      "12/12 [==============================] - 1s 122ms/step - loss: 1.9687 - accuracy: 0.2745 - val_loss: 2.0622 - val_accuracy: 0.2708\n",
      "Epoch 11/40\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 1.8045 - accuracy: 0.3967 - val_loss: 1.8027 - val_accuracy: 0.3854\n",
      "Epoch 12/40\n",
      "12/12 [==============================] - 1s 101ms/step - loss: 1.7286 - accuracy: 0.3859 - val_loss: 1.5414 - val_accuracy: 0.4375\n",
      "Epoch 13/40\n",
      "12/12 [==============================] - 1s 109ms/step - loss: 1.6034 - accuracy: 0.4266 - val_loss: 1.6319 - val_accuracy: 0.3333\n",
      "Epoch 14/40\n",
      "12/12 [==============================] - 1s 106ms/step - loss: 1.5772 - accuracy: 0.4674 - val_loss: 1.3424 - val_accuracy: 0.5833\n",
      "Epoch 15/40\n",
      "12/12 [==============================] - 1s 100ms/step - loss: 1.4261 - accuracy: 0.5217 - val_loss: 1.0929 - val_accuracy: 0.6667\n",
      "Epoch 16/40\n",
      "12/12 [==============================] - 1s 112ms/step - loss: 1.4529 - accuracy: 0.4810 - val_loss: 1.0002 - val_accuracy: 0.6667\n",
      "Epoch 17/40\n",
      "12/12 [==============================] - 1s 116ms/step - loss: 1.4070 - accuracy: 0.5326 - val_loss: 0.9799 - val_accuracy: 0.6562\n",
      "Epoch 18/40\n",
      "12/12 [==============================] - 1s 106ms/step - loss: 1.3451 - accuracy: 0.5408 - val_loss: 1.2357 - val_accuracy: 0.5938\n",
      "Epoch 19/40\n",
      "12/12 [==============================] - 2s 131ms/step - loss: 1.2477 - accuracy: 0.5462 - val_loss: 1.8744 - val_accuracy: 0.4062\n",
      "Epoch 20/40\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 1.2673 - accuracy: 0.5734 - val_loss: 1.0705 - val_accuracy: 0.6250\n",
      "Epoch 21/40\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 1.1083 - accuracy: 0.6250 - val_loss: 1.0485 - val_accuracy: 0.5938\n",
      "Epoch 22/40\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 1.0423 - accuracy: 0.6685 - val_loss: 0.5914 - val_accuracy: 0.8229\n",
      "Epoch 23/40\n",
      "12/12 [==============================] - 2s 133ms/step - loss: 1.1129 - accuracy: 0.6413 - val_loss: 0.6648 - val_accuracy: 0.8229\n",
      "Epoch 24/40\n",
      "12/12 [==============================] - 2s 139ms/step - loss: 1.1482 - accuracy: 0.5842 - val_loss: 0.5941 - val_accuracy: 0.8646\n",
      "Epoch 25/40\n",
      "12/12 [==============================] - 2s 140ms/step - loss: 0.9913 - accuracy: 0.6576 - val_loss: 0.6347 - val_accuracy: 0.8646\n",
      "Epoch 26/40\n",
      "12/12 [==============================] - 2s 122ms/step - loss: 0.9525 - accuracy: 0.6875 - val_loss: 0.7038 - val_accuracy: 0.8021\n",
      "Epoch 27/40\n",
      "12/12 [==============================] - 2s 130ms/step - loss: 0.8516 - accuracy: 0.7446 - val_loss: 0.6101 - val_accuracy: 0.8542\n",
      "Epoch 28/40\n",
      "12/12 [==============================] - 2s 132ms/step - loss: 0.8989 - accuracy: 0.7418 - val_loss: 0.8465 - val_accuracy: 0.7188\n",
      "Epoch 29/40\n",
      "12/12 [==============================] - 2s 175ms/step - loss: 0.8828 - accuracy: 0.6957 - val_loss: 0.6584 - val_accuracy: 0.7917\n",
      "Epoch 30/40\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 0.8735 - accuracy: 0.7038 - val_loss: 0.8210 - val_accuracy: 0.7604\n",
      "Epoch 31/40\n",
      "12/12 [==============================] - 2s 139ms/step - loss: 0.8865 - accuracy: 0.6902 - val_loss: 0.4356 - val_accuracy: 0.9062\n",
      "Epoch 32/40\n",
      "12/12 [==============================] - 1s 106ms/step - loss: 0.8310 - accuracy: 0.7609 - val_loss: 0.7364 - val_accuracy: 0.7708\n",
      "Epoch 33/40\n",
      "12/12 [==============================] - 1s 103ms/step - loss: 0.8287 - accuracy: 0.7201 - val_loss: 0.5550 - val_accuracy: 0.8021\n",
      "Epoch 34/40\n",
      "12/12 [==============================] - 1s 121ms/step - loss: 0.8658 - accuracy: 0.7011 - val_loss: 0.6563 - val_accuracy: 0.8229\n",
      "Epoch 35/40\n",
      "12/12 [==============================] - 1s 102ms/step - loss: 0.7418 - accuracy: 0.7500 - val_loss: 0.4058 - val_accuracy: 0.8750\n",
      "Epoch 36/40\n",
      "12/12 [==============================] - 2s 137ms/step - loss: 0.8117 - accuracy: 0.7228 - val_loss: 0.5678 - val_accuracy: 0.8542\n",
      "Epoch 37/40\n",
      "12/12 [==============================] - 2s 137ms/step - loss: 0.7684 - accuracy: 0.7473 - val_loss: 0.4444 - val_accuracy: 0.8750\n",
      "Epoch 38/40\n",
      "12/12 [==============================] - 2s 127ms/step - loss: 0.7798 - accuracy: 0.7473 - val_loss: 0.5696 - val_accuracy: 0.8229\n",
      "Epoch 39/40\n",
      "12/12 [==============================] - 2s 162ms/step - loss: 0.7481 - accuracy: 0.7554 - val_loss: 1.0091 - val_accuracy: 0.6667\n",
      "Epoch 40/40\n",
      "12/12 [==============================] - 2s 147ms/step - loss: 0.7774 - accuracy: 0.7228 - val_loss: 0.6154 - val_accuracy: 0.8333\n",
      "4/4 - 0s - loss: 0.5942 - accuracy: 0.8400 - 302ms/epoch - 75ms/step\n",
      "Test accuracy: 0.8399999737739563\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define your custom dataset paths\n",
    "train_dir = r'C:\\Users\\User\\Downloads\\DataSet\\Train'\n",
    "test_dir = r'C:\\Users\\User\\Downloads\\DataSet\\Test'\n",
    "\n",
    "# Define image dimensions and batch size\n",
    "img_width, img_height = 28, 28\n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation for training data (optional but recommended)\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0 / 255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=False,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'  # Set to 'categorical' for multiple classes\n",
    ")\n",
    "\n",
    "# Load and preprocess the test data\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'  # Set to 'categorical' for multiple classes\n",
    ")\n",
    "\n",
    "# Define the number of classes in your dataset\n",
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.5),  # Dropout layer to reduce overfitting\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "epochs = 40  # You can adjust the number of epochs as needed\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=test_generator.samples // batch_size\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(test_generator, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7c93d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_3 (Conv2D)           (None, 26, 26, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 13, 13, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPoolin  (None, 5, 5, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPoolin  (None, 1, 1, 128)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 111050 (433.79 KB)\n",
      "Trainable params: 111050 (433.79 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97a383f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has successfully trained\n",
      "Saving the model as custom.h5\n"
     ]
    }
   ],
   "source": [
    "print(\"The model has successfully trained\")\n",
    "model.save('custom.h5')\n",
    "print(\"Saving the model as custom.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee9000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import tkinter as tk\n",
    "import win32gui\n",
    "from PIL import ImageGrab, Image\n",
    "import numpy as np\n",
    "\n",
    "model = load_model('custom.h5')\n",
    "\n",
    "def predict_digit(img):\n",
    "    # Resize image to 28x28 pixels\n",
    "    img = img.resize((28, 28))\n",
    "    # Convert RGB to grayscale\n",
    "    img = img.convert('L')\n",
    "    # Convert grayscale image to three-channel (RGB)\n",
    "    img = Image.merge('RGB', (img, img, img))\n",
    "    img = np.array(img)\n",
    "    # Reshape for model normalization\n",
    "    img = img.reshape(1, 28, 28, 3)  # Use 3 channels\n",
    "    img = img / 255.0\n",
    "    # Predict the digit\n",
    "    res = model.predict([img])[0]\n",
    "    return np.argmax(res), max(res)\n",
    "\n",
    "class App(tk.Tk):\n",
    "    def __init__(self):\n",
    "        tk.Tk.__init__(self)\n",
    "        self.x = self.y = 0\n",
    "        # Creating elements\n",
    "        self.canvas = tk.Canvas(self, width=200, height=200, bg=\"white\", cursor=\"cross\")\n",
    "        self.label = tk.Label(self, text=\"Analyzing..\", font=(\"Helvetica\", 48))\n",
    "        self.classify_btn = tk.Button(self, text=\"Search\", command=self.classify_handwriting)\n",
    "        self.button_clear = tk.Button(self, text=\"Clear\", command=self.clear_all)\n",
    "        # Grid structure\n",
    "        self.canvas.grid(row=0, column=0, pady=2, sticky=tk.W)\n",
    "        self.label.grid(row=0, column=1, pady=2, padx=2)\n",
    "        self.classify_btn.grid(row=1, column=1, pady=2, padx=2)\n",
    "        self.button_clear.grid(row=1, column=0, pady=2)\n",
    "        self.canvas.bind(\"<Button-1>\", self.start_pos)\n",
    "        self.canvas.bind(\"<B1-Motion>\", self.draw_lines)\n",
    "\n",
    "    def clear_all(self):\n",
    "        self.canvas.delete(\"all\")\n",
    "\n",
    "    def classify_handwriting(self):\n",
    "        hd = self.canvas.winfo_id()  # to fetch the handle of the canvas\n",
    "        rect = win32gui.GetWindowRect(hd)  # to fetch the edges of the canvas\n",
    "        im = ImageGrab.grab(rect)\n",
    "        digit, acc = predict_digit(im)\n",
    "        self.label.configure(text=str(digit) + ', ' + str(int(acc * 100)) + '%')\n",
    "\n",
    "    def start_pos(self, event):\n",
    "        self.x = event.x\n",
    "        self.y = event.y\n",
    "\n",
    "    def draw_lines(self, event):\n",
    "        x1, y1 = (event.x - 1), (event.y - 1)\n",
    "        x2, y2 = (event.x + 1), (event.y + 1)\n",
    "        self.canvas.create_oval(x1, y1, x2, y2, fill='white', width=5)\n",
    "\n",
    "app = App()\n",
    "app.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c010d2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
